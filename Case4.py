# -*- coding: utf-8 -*-
"""Case4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KCo70gG7L74p0zQunZoAX9QXW7Utsy_x

#**Importação de Bibliotecas e Conjunto de Dados**

Instalação de Bibliotecas
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install plotly
# %pip install cufflinks
# %pip install chart-studio

!pip install pandas-profiling
!pip install sidetable
!pip install scikit-learn

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# %pip install KModes

"""Importação das principais bilbiotecas ultilizadas"""

import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt

import chart_studio.plotly as py
import cufflinks as cf

import plotly.graph_objects as go
import plotly.express as px

import missingno as msno
from ipywidgets import interact, widgets

from sklearn import datasets
from sklearn.preprocessing import scale, minmax_scale, power_transform
from sklearn.datasets import load_wine

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

from sklearn.metrics import confusion_matrix
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, classification_report

from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer, kelbow_visualizer, silhouette_visualizer
from sklearn.mixture import GaussianMixture
from sklearn.cluster import KMeans, DBSCAN, MeanShift
from sklearn.cluster import k_means, dbscan, mean_shift, estimate_bandwidth
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from sklearn.decomposition import PCA
from sklearn.preprocessing import scale

from kmodes.kmodes import KModes
from kmodes.kprototypes import KPrototypes

import string
from ipywidgets import interact

from imblearn.over_sampling import SMOTE

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

from sklearn.metrics import roc_curve, roc_auc_score

"""Carregue o conjunto de dados"""

df = pd.read_csv('/content/BankChurner - ATT.csv', delimiter=';')

"""Exibir dados"""

df.head()

"""#**Tratamento dos Dados**

Estrutura do conjunto de dado
"""

df.info()

df.dtypes

"""Dados duplicados"""

duplicados = df.duplicated().sum()

df_sem_duplicatas = df.drop_duplicates()

"""#**Limpeza dos Dados**

Valores ausentes
"""

df.isnull().sum()

"""Inconsistências nos dados"""

df.dtypes

df = df.astype({"CLIENTNUM":"int", "Attrition_Flag":"str",
                 "Customer_Age":"int","Gender":"str",
                 "Dependent_count":"int","Education_Level":"str"})
df.dtypes

df.head()

"""#**Análise Exploratória de Dados (EDA)**

Visualização da Variável Dependente
"""

sns.countplot(x='Attrition_Flag', data=df)
plt.title('Distribuição de Churn')
plt.xlabel('Churn')
plt.ylabel('Contagem')
plt.show()

df['Attrition_Flag'].value_counts().plot.pie(autopct='%1.1f%%', colors=['skyblue', 'salmon'], startangle=90, figsize=(7,7))
plt.title('Proporção de Churn')
plt.ylabel('')
plt.show()

"""Visualização das Variáveis Independentes"""

df.hist(bins=30, figsize=(10,10), color='skyblue')
plt.suptitle('Distribuição das Variáveis Numéricas')
plt.show()

"""#**Pré-processamento dos Dados**

Seleção e Importância de Variáveis
"""

y = df['Attrition_Flag']
X = df.drop(columns=['Attrition_Flag'])

X = pd.get_dummies(X)

model = RandomForestClassifier()
model.fit(X, y)

importances = model.feature_importances_
feature_names = X.columns
importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances}).sort_values(by='Importance', ascending=False)

top_n = 10  # Número de variáveis que você quer visualizar
top_features = importance_df.head(top_n)

# Gerar o gráfico para as 20 variáveis principais
top_features.plot(kind='barh', x='Feature', y='Importance', title='Importância das Variáveis (Top 10)', figsize=(10, 6))
plt.show()

"""Transformação de Características, Dimensionamento e Codificação"""

scaler = MinMaxScaler()
X_scaler = scaler.fit_transform(X)

"""Divisão dos Dados para Treinamento do Modelo"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

"""Aplicação do SMOTE para reduzir desequilíbrio de classes"""

smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

print(y_train_resampled.value_counts())

"""#**Criação, Treinamento e Avaliação do Modelo**

Seleção de Algoritmos de Classificação
"""

classifiers = {
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier(),
    'SVM': SVC(probability=True)
}

"""Treinamento e Ajuste do Modelo"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

for name, clf in classifiers.items():
    clf.fit(X_train, y_train)
    print(f"{name} model trained successfully")

"""Avaliação do Modelo e Desempenho"""

for name, clf in classifiers.items():
    y_pred = clf.predict(X_test)
    print(f"\n{name} Model Evaluation:")
    print(f"Acurácia: {accuracy_score(y_test, y_pred):.4f}")
    print(f"Precisão: {precision_score(y_test, y_pred, pos_label='Existing Customer'):.4f}") # Set appropriate pos_label
    print(f"Recall: {recall_score(y_test, y_pred, pos_label='Existing Customer'):.4f}") # Set appropriate pos_label
    print(f"F1-Score: {f1_score(y_test, y_pred, pos_label='Existing Customer'):.4f}") # Set appropriate pos_label

"""Análise da Matriz de Confusão"""

for name, clf in classifiers.items():
    y_pred = clf.predict(X_test)
    cm = confusion_matrix(y_test, y_pred)

    plt.figure(figsize=(6,4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Matriz de Confusão - {name}')
    plt.xlabel('Previsto')
    plt.ylabel('Real')
    plt.show()

"""Curva ROC (Receiver Operating Characteristic) e AUC"""

for name, clf in classifiers.items():
    y_pred_prob = clf.predict_proba(X_test)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, y_pred_prob, pos_label='Existing Customer') # Added pos_label argument

    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc_score(y_test, y_pred_prob):.4f})')

plt.plot([0, 1], [0, 1], 'k--')  # Linha de referência
plt.title('Curva ROC')
plt.xlabel('Taxa de Falsos Positivos')
plt.ylabel('Taxa de Verdadeiros Positivos')
plt.legend()
plt.show()

"""Importância e Contribuição das Características"""

importances = classifiers['Random Forest'].feature_importances_
feature_names = X.columns
importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances}).sort_values(by='Importance', ascending=False)

top_features = importance_df.head(10)
top_features.plot(kind='barh', x='Feature', y='Importance', title='Importância das Variáveis')
plt.show()
